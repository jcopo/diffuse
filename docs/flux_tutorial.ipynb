{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX.1-dev with Diffuse: Modular Sampling Framework\n",
    "\n",
    "This notebook demonstrates how to use FLUX.1-dev {cite}`flux2024` with the **Diffuse** sampling framework - showcasing the power of researching sampling algorithms WITHOUT dealing with training complexity.\n",
    "\n",
    "## The Research Opportunity\n",
    "\n",
    "Most exciting diffusion research isn't about training models - it's about the **algorithms built on top**:\n",
    "\n",
    "- \ud83c\udfa8 **Image editing** (InstructPix2Pix, DiffEdit, Imagic)\n",
    "- \ud83d\uddbc\ufe0f **Inpainting & outpainting** (RePaint, Blended Diffusion)\n",
    "- \ud83d\udd0d **Inverse problems** (DPS, RED-diff, medical imaging)\n",
    "- \ud83c\udfaf **Controllable generation** (ControlNet-style, regional control)\n",
    "- \ud83d\udcd0 **Novel sampling methods** (better integrators, adaptive schedules)\n",
    "- \u26a1 **Distillation & acceleration** (consistency models, few-step sampling)\n",
    "\n",
    "All of these need **modular sampling infrastructure**. But most researchers rebuild it from scratch every time.\n",
    "\n",
    "**Diffuse solves this**: Load pre-trained models \u2192 Experiment with sampling \u2192 Focus on YOUR research.\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **Flow Matching Models** - Understanding FLUX as a velocity field predictor\n",
    "2. **Modular Components** - Swap timers, integrators, guidance without touching model code\n",
    "3. **Quality Comparisons** - Visual side-by-side across configurations\n",
    "4. **Stochastic vs Deterministic** - Adding controlled randomness for diversity\n",
    "5. **Research Velocity** - Test ideas in hours, not weeks\n",
    "\n",
    "## What is FLUX.1-dev?\n",
    "\n",
    "**FLUX.1-dev** {cite}`flux2024` is a state-of-the-art text-to-image model trained using **flow matching** {cite}`Liu2022` {cite}`Lipman2022` (also called rectified flow). Unlike traditional diffusion models that learn to denoise images, FLUX learns a **velocity field** that transforms noise into images along straight paths.\n",
    "\n",
    "### Flow Matching Background\n",
    "\n",
    "As detailed in the [Diffusion Crash Course](diffusion_crash_course.rst), flow matching uses the straight-line interpolation path:\n",
    "\n",
    "$$\n",
    "x_t = (1-t)x_0 + t\\varepsilon, \\quad \\varepsilon\\sim\\mathcal{N}(0,I), \\quad t \\in [0, 1]\n",
    "$$\n",
    "\n",
    "where $x_0$ is clean data and $\\varepsilon$ is Gaussian noise. FLUX {cite}`flux2024` learns a **velocity field** $v_\\theta(x_t, t, c)$ conditioned on text embeddings $c$ that defines the flow ODE (see Eq. :eq:`eq:flow_ode` in the crash course):\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = v_\\theta(x_t, t, c)\n",
    "$$\n",
    "\n",
    "### Why This Matters for Research\n",
    "\n",
    "This ODE formulation means you can:\n",
    "- \u2705 **Swap integrators** (Euler, Heun, DPM++, DDIM) without retraining\n",
    "- \u2705 **Change discretization schedules** (uniform, adaptive, learned)\n",
    "- \u2705 **Add stochasticity** (churning, noise injection) for diversity\n",
    "- \u2705 **Apply to inverse problems** (inpainting, super-resolution, deblurring)\n",
    "- \u2705 **Compose guidance methods** (classifier-free, DPS, custom)\n",
    "\n",
    "**No model training needed.** Just load weights and experiment with sampling.\n",
    "\n",
    "**Diffuse** makes this modular and easy. That's the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path",
    "import numpy as np",
    "import matplotlib.pyplot as plt",
    "from matplotlib.gridspec import GridSpec",
    "",
    "import jax",
    "import jax.numpy as jnp",
    "from PIL import Image",
    "",
    "# FLUX components (standalone - no triax dependency!)",
    "from diffuse.examples.flux_dev.run_flux_inference import (",
    "    FluxModelLoader,",
    "    FluxConditionedNetwork,",
    "    _latent_shapes,",
    ")",
    "from diffuse.examples.flux_dev.utils import FluxTimer",
    "",
    "# Diffuse components",
    "from diffuse.denoisers.denoiser import Denoiser",
    "from diffuse.diffusion.sde import Flow",
    "from diffuse.integrator.deterministic import (",
    "    DDIMIntegrator,",
    "    EulerIntegrator,",
    "    HeunIntegrator,",
    "    DPMpp2sIntegrator,",
    ")",
    "from diffuse.integrator.stochastic import EulerMaruyamaIntegrator",
    "from diffuse.predictor import Predictor",
    "from diffuse.timer.base import VpTimer",
    "",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set paths and generation parameters. Update `CHECKPOINT_DIR` to point to your FLUX model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - UPDATE THIS TO MATCH YOUR SETUP\n",
    "CHECKPOINT_DIR = Path(\"/path/to/flux/checkpoint\")  # Contains transformer/vae/clip_text/t5_text\n",
    "\n",
    "# Generation parameters\n",
    "PROMPT = \"A serene landscape with mountains at sunset, highly detailed, photorealistic\"\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "NUM_STEPS = 20\n",
    "GUIDANCE_SCALE = 4.0\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Prompt: {PROMPT}\")\n",
    "print(f\"Resolution: {WIDTH}x{HEIGHT}\")\n",
    "print(f\"Steps: {NUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FLUX Model\n",
    "\n",
    "This section demonstrates Diffuse's **separation of concerns** - one of the key design principles that enables rapid experimentation.\n",
    "\n",
    "### The Three Stages\n",
    "\n",
    "1. **Model Loading** - `FluxModelLoader` handles checkpoints, tokenizers, text encoders  \n",
    "2. **Text Conditioning** - `prepare_conditioned_network` encodes prompt, returns velocity field  \n",
    "3. **Sampling** - Modular components (Timer, Integrator, Denoiser) handle generation\n",
    "\n",
    "### What does `prepare_conditioned_network` do?\n",
    "\n",
    "FLUX was trained using flow matching to predict **velocity fields**. This function:\n",
    "\n",
    "- **Tokenizes your prompt** using CLIP and T5 tokenizers  \n",
    "- **Encodes text** through CLIP (pooled embeddings) and T5 (sequence embeddings)\n",
    "- **Loads the FLUX transformer** onto GPU\n",
    "- **Returns a conditioned velocity field** `v(x_t, t)` - a function mapping (latents + time) \u2192 velocities\n",
    "\n",
    "Think of it as \"baking\" your text prompt into the velocity field. Now you can sample from this field using **any integrator** without touching text encoders again.\n",
    "\n",
    "### Why This Design Matters for Research\n",
    "\n",
    "- \u2705 **Text encoders offloaded** after encoding (saves memory)\n",
    "- \u2705 **Clean velocity field interface** for sampling experiments\n",
    "- \u2705 **Swap sampling components** without re-encoding  \n",
    "- \u2705 **Zero loading complexity** for your research code\n",
    "\n",
    "This is the \"simple pipeline, no loading craziness\" philosophy in action. You focus on sampling algorithms, not infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loader = FluxModelLoader(checkpoint_dir=CHECKPOINT_DIR, verbose=True)\n",
    "\n",
    "# Prepare conditioned velocity field\n",
    "conditioned = loader.prepare_conditioned_network(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=None,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    ")\n",
    "\n",
    "print(f\"\\nConditioned network ready (dtype={conditioned.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions: Modular Sampling with Diffuse\n",
    "\n",
    "The key to Diffuse's power is **separation of concerns**. Each component has **one job**:\n",
    "\n",
    "- **Timer**: Decides WHEN to evaluate the model (discretization schedule)\n",
    "- **Predictor**: Wraps the velocity field `v(x_t, t)` from FLUX\n",
    "- **Integrator**: Decides HOW to step from `x_t` to `x_{t-1}` (numerical ODE solver)\n",
    "- **Denoiser**: Orchestrates the full sampling loop\n",
    "\n",
    "This modularity is the entire point: **change one component, everything else stays the same**.\n",
    "\n",
    "### The No-Training Research Paradigm\n",
    "\n",
    "Want to test a new integrator idea? Just swap one line.  \n",
    "Want to try a different schedule? Just swap one line.  \n",
    "Want to add guidance? Just swap the Denoiser.\n",
    "\n",
    "No model retraining. No loading spaghetti. **Just your research idea**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(\n",
    "    conditioned_network: FluxConditionedNetwork,\n",
    "    timer,\n",
    "    integrator_class,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    num_steps: int,\n",
    "    seed: int,\n",
    ") -> jax.Array:\n",
    "    \"\"\"Generate image using modular Diffuse components.\"\"\"\n",
    "    _, transformer_hw = _latent_shapes(height, width)\n",
    "    image_seq_len = transformer_hw[0] * transformer_hw[1]\n",
    "\n",
    "    # Set dynamic shift for FluxTimer if needed\n",
    "    if isinstance(timer, FluxTimer) and timer.use_dynamic_shift:\n",
    "        timer.set_image_seq_len(image_seq_len)\n",
    "\n",
    "    # Modular component assembly\n",
    "    flow = Flow(tf=1.0)\n",
    "    predictor = Predictor(\n",
    "        model=flow,\n",
    "        network=conditioned_network.network_fn,\n",
    "        prediction_type=\"velocity\",\n",
    "    )\n",
    "    integrator = integrator_class(model=flow, timer=timer)\n",
    "    denoiser = Denoiser(\n",
    "        integrator=integrator,\n",
    "        model=flow,\n",
    "        predictor=predictor,\n",
    "        x0_shape=(transformer_hw[0], transformer_hw[1], conditioned_network.in_channels),\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    state, _ = denoiser.generate(\n",
    "        rng_key=key,\n",
    "        n_steps=num_steps,\n",
    "        n_particles=1,\n",
    "        keep_history=False,\n",
    "    )\n",
    "    return state.integrator_state.position.astype(conditioned_network.dtype)\n",
    "\n",
    "\n",
    "def decode_and_display(latents: jax.Array, loader: FluxModelLoader) -> np.ndarray:\n",
    "    \"\"\"Decode latents to RGB image.\"\"\"\n",
    "    images = loader.decode_latents(latents)\n",
    "    return images[0]\n",
    "\n",
    "\n",
    "def plot_comparison(images: dict, title: str, figsize=(15, 10)):\n",
    "    \"\"\"Plot grid of images for comparison.\"\"\"\n",
    "    n = len(images)\n",
    "    cols = min(3, n)\n",
    "    rows = (n + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, (name, img) in enumerate(images.items()):\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(name, fontsize=12, fontweight=\"bold\")\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    for idx in range(len(images), len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=16, fontweight=\"bold\", y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Timer Comparison\n",
    "\n",
    "Timers control the **time discretization** $t \\in [0, 1]$. Different schedules allocate more steps to different noise levels.\n",
    "\n",
    "### VpTimer\n",
    "Linear discretization: $t_i = t_f + \\frac{i}{N}(\\epsilon - t_f)$\n",
    "\n",
    "### FluxTimer\n",
    "Applies a M\u00f6bius transformation to bias sampling toward low-noise regions:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{shifted}}(t) = \\frac{\\mu \\cdot \\sigma(t)}{1 + (\\mu - 1) \\cdot \\sigma(t)}\n",
    "$$\n",
    "\n",
    "- **Static mode**: Fixed $\\mu = 1.15$ (FLUX default)\n",
    "- **Dynamic mode**: Resolution-adaptive $\\mu(L)$ based on sequence length $L$\n",
    "\n",
    "The M\u00f6bius shift allocates more steps to fine details (low noise), improving quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timers\n",
    "vp_timer = VpTimer(n_steps=NUM_STEPS, eps=1e-3, tf=1.0)\n",
    "flux_timer_static = FluxTimer(\n",
    "    n_steps=NUM_STEPS, eps=1e-3, tf=1.0, shift=1.15, use_dynamic_shift=False\n",
    ")\n",
    "flux_timer_dynamic = FluxTimer(\n",
    "    n_steps=NUM_STEPS, eps=1e-3, tf=1.0, shift=1.15, use_dynamic_shift=True\n",
    ")\n",
    "\n",
    "# Visualize schedules\n",
    "_, transformer_hw = _latent_shapes(HEIGHT, WIDTH)\n",
    "image_seq_len = transformer_hw[0] * transformer_hw[1]\n",
    "flux_timer_dynamic.set_image_seq_len(image_seq_len)\n",
    "\n",
    "steps = np.arange(NUM_STEPS + 1)\n",
    "vp_schedule = [vp_timer(s) for s in steps]\n",
    "flux_static_schedule = [flux_timer_static(s) for s in steps]\n",
    "flux_dynamic_schedule = [flux_timer_dynamic(s) for s in steps]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, vp_schedule, \"o-\", label=\"VpTimer\", linewidth=2)\n",
    "plt.plot(steps, flux_static_schedule, \"s-\", label=r\"FluxTimer (static, $\\mu=1.15$)\", linewidth=2)\n",
    "plt.plot(steps, flux_dynamic_schedule, \"^-\", label=f\"FluxTimer (dynamic, $\\\\mu={flux_timer_dynamic._mu:.3f}$)\", linewidth=2)\n",
    "plt.xlabel(\"Step $i$\", fontsize=12)\n",
    "plt.ylabel(\"Time $t_i$\", fontsize=12)\n",
    "plt.title(\"Timer Schedules\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "diff_static = np.diff(flux_static_schedule)\n",
    "diff_dynamic = np.diff(flux_dynamic_schedule)\n",
    "diff_vp = np.diff(vp_schedule)\n",
    "plt.plot(steps[:-1], -diff_vp, \"o-\", label=\"VpTimer\", linewidth=2)\n",
    "plt.plot(steps[:-1], -diff_static, \"s-\", label=\"FluxTimer (static)\", linewidth=2)\n",
    "plt.plot(steps[:-1], -diff_dynamic, \"^-\", label=\"FluxTimer (dynamic)\", linewidth=2)\n",
    "plt.xlabel(\"Step $i$\", fontsize=12)\n",
    "plt.ylabel(\"Step Size $-\\\\Delta t_i$\", fontsize=12)\n",
    "plt.title(\"Step Size Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResolution: {WIDTH}x{HEIGHT} \u2192 {image_seq_len} tokens\")\n",
    "print(f\"Dynamic shift \u03bc: {flux_timer_dynamic._mu:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with Different Timers\n",
    "\n",
    "We use `DDIMIntegrator` {cite}`Song2020b` (see crash course section on DDIM) to isolate the effect of the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating with VpTimer...\")\n",
    "latents_vp = run_generation(conditioned, vp_timer, DDIMIntegrator, HEIGHT, WIDTH, NUM_STEPS, SEED)\n",
    "img_vp = decode_and_display(latents_vp, loader)\n",
    "\n",
    "print(\"Generating with FluxTimer (static)...\")\n",
    "latents_flux_static = run_generation(\n",
    "    conditioned, flux_timer_static, DDIMIntegrator, HEIGHT, WIDTH, NUM_STEPS, SEED\n",
    ")\n",
    "img_flux_static = decode_and_display(latents_flux_static, loader)\n",
    "\n",
    "print(\"Generating with FluxTimer (dynamic)...\")\n",
    "latents_flux_dynamic = run_generation(\n",
    "    conditioned, flux_timer_dynamic, DDIMIntegrator, HEIGHT, WIDTH, NUM_STEPS, SEED\n",
    ")\n",
    "img_flux_dynamic = decode_and_display(latents_flux_dynamic, loader)\n",
    "\n",
    "timer_images = {\n",
    "    \"VpTimer\": img_vp,\n",
    "    \"FluxTimer (Static)\": img_flux_static,\n",
    "    \"FluxTimer (Dynamic)\": img_flux_dynamic,\n",
    "}\n",
    "\n",
    "plot_comparison(\n",
    "    timer_images,\n",
    "    f\"Timer Comparison (DDIM, {NUM_STEPS} steps)\",\n",
    "    figsize=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Differences:**\n",
    "- **VpTimer**: Uniform steps, may miss fine details\n",
    "- **FluxTimer (Static)**: More low-noise steps, better details\n",
    "- **FluxTimer (Dynamic)**: Resolution-adaptive, optimal for multi-resolution workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Integrator Comparison\n",
    "\n",
    "### What Are Integrators?\n",
    "\n",
    "FLUX gives us a velocity field `v(x_t, t)` - it tells us \"which direction to move\" at each point. An **integrator** is the numerical method we use to follow that velocity field from noise (t=1) to image (t=0).\n",
    "\n",
    "Think of it like GPS navigation:\n",
    "- **Velocity field** = GPS telling you \"turn left, go straight, turn right\"  \n",
    "- **Integrator** = how you follow those directions (fast but imprecise vs slow but accurate)\n",
    "\n",
    "### Why Integrators Matter for Quality\n",
    "\n",
    "Different integrators can significantly affect **visual quality**. For example:\n",
    "- DPM++2S often produces **sharper reflections** (like sunlight on water)\n",
    "- Heun preserves **fine texture details** better than Euler\n",
    "- Euler-Maruyama adds **controlled stochasticity** for diverse samples\n",
    "\n",
    "This is pure sampling research - no model retraining, just algorithmic improvements.\n",
    "\n",
    "### Integrators We'll Compare\n",
    "\n",
    "**DDIM** {cite}`Song2020b` (see crash course section on DDIM)\n",
    "- Fast, deterministic, well-tested\n",
    "- Good default choice for most applications\n",
    "\n",
    "**Euler** (First-Order)\n",
    "- Simplest: just follow the velocity directly\n",
    "- Fast but can accumulate errors over many steps  \n",
    "- Good for quick previews\n",
    "\n",
    "**Heun** (Second-Order)\n",
    "- \"Look ahead\" method: predicts next step, corrects itself\n",
    "- 2x slower (two model evaluations per step) but more accurate\n",
    "- Better detail preservation\n",
    "\n",
    "**DPM++2S** {cite}`Lu2022` (Second-Order Optimized)\n",
    "- Like Heun but with optimized stability\n",
    "- Works in log-space for better numerical precision\n",
    "- Often produces best quality - **notice water reflections, fine textures**\n",
    "\n",
    "**Euler-Maruyama** (Stochastic)\n",
    "- Adds controlled noise during sampling\n",
    "- Generates diverse samples even with same seed\n",
    "- Great for exploration and multiple variations\n",
    "\n",
    "### Key Insight: Quality vs Speed Trade-off\n",
    "\n",
    "- **Same steps**: Heun \u2248 DPM++2S > DDIM > Euler (quality)\n",
    "- **Same compute**: DDIM at 40 steps \u2248 Heun at 20 steps\n",
    "- **For FLUX**: DPM++2S often produces noticeably better fine details\n",
    "\n",
    "Let's see the visual differences using FluxTimer (the default FLUX schedule):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FluxTimer for fair comparison\n",
    "timer_for_comparison = FluxTimer(\n",
    "    n_steps=NUM_STEPS, eps=1e-3, tf=1.0, shift=1.15, use_dynamic_shift=False\n",
    ")\n",
    "\n",
    "integrators = [\n",
    "    (\"DDIM\", DDIMIntegrator),\n",
    "    (\"Euler\", EulerIntegrator),\n",
    "    (\"Heun\", HeunIntegrator),\n",
    "    (\"DPM++2S\", DPMpp2sIntegrator),\n",
    "    (\"Euler-Maruyama\", EulerMaruyamaIntegrator),\n",
    "]\n",
    "\n",
    "integrator_images = {}\n",
    "\n",
    "for name, integrator_class in integrators:\n",
    "    print(f\"Generating with {name}...\")\n",
    "    latents = run_generation(\n",
    "        conditioned, timer_for_comparison, integrator_class, HEIGHT, WIDTH, NUM_STEPS, SEED\n",
    "    )\n",
    "    img = decode_and_display(latents, loader)\n",
    "    integrator_images[name] = img\n",
    "\n",
    "plot_comparison(\n",
    "    integrator_images,\n",
    "    f\"Integrator Comparison (FluxTimer, {NUM_STEPS} steps)\",\n",
    "    figsize=(20, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Look For\n",
    "- **DDIM**: Solid baseline, clean images\n",
    "- **Euler**: Slightly softer details\n",
    "- **Heun**: Sharper than Euler\n",
    "- **DPM++2S**: Often best quality (reflections, textures)\n",
    "- **Euler-Maruyama**: Stochastic diversity\n",
    "\n",
    "### Recommendations\n",
    "- **Fast previews**: Euler or DDIM\n",
    "- **Balanced**: DDIM\n",
    "- **Max quality**: DPM++2S or Heun\n",
    "- **Diversity**: Euler-Maruyama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Inverse Problems with DPS\n",
    "\n",
    "So far we've done **unconditional generation** - going from noise to images guided only by text prompts. But one of the most powerful research areas is solving **inverse problems**: recovering clean images from degraded observations.\n",
    "\n",
    "### What are Inverse Problems?\n",
    "\n",
    "An inverse problem has this form:\n",
    "\n",
    "$$\n",
    "y = A(x) + n\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the clean image we want to recover\n",
    "- $A$ is a measurement operator (downsampling, masking, blurring, etc.)\n",
    "- $y$ is our observed/degraded image\n",
    "- $n$ is measurement noise\n",
    "\n",
    "**Common examples:**\n",
    "- **Inpainting**: $A$ masks out pixels \u2192 recover missing regions\n",
    "- **Super-resolution**: $A$ downsamples image \u2192 recover high-res details\n",
    "- **Deblurring**: $A$ applies blur kernel \u2192 recover sharp image\n",
    "- **Compressed sensing**: $A$ is random projection \u2192 recover from few measurements\n",
    "- **Medical imaging**: $A$ is CT/MRI physics \u2192 reconstruct from k-space\n",
    "\n",
    "### How DPS Works {cite}`Chung2022`\n",
    "\n",
    "DPS (Diffusion Posterior Sampling) elegantly solves inverse problems by:\n",
    "\n",
    "1. **Taking a normal diffusion step** using the integrator (as if unconditional)\n",
    "2. **Checking measurement consistency**: \"Does our current estimate match the observed data $y$?\"\n",
    "3. **Applying a gradient correction** to push toward measurement-consistent solutions\n",
    "\n",
    "Mathematically: $x_{t-1} = \\text{step}(x_t, t) - \\zeta \\nabla_x \\|y - A(x_t)\\|^2$\n",
    "\n",
    "The beauty: **you can use any integrator + any measurement operator without retraining**!\n",
    "\n",
    "### Why This Matters for Research\n",
    "\n",
    "This is THE example of \"algorithms on top of models\" research. With Diffuse:\n",
    "- \u2705 Implement your measurement operator (20 lines of code)\n",
    "- \u2705 Plug into DPS with any integrator\n",
    "- \u2705 Test immediately on FLUX (or any model)\n",
    "- \u2705 Compare against baselines instantly\n",
    "\n",
    "No training. No model modification. Just your inverse problem algorithm.\n",
    "\n",
    "Let's see it in action with two examples: **inpainting** and **super-resolution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DPS denoiser\n",
    "from diffuse.denoisers.cond import DPSDenoiser\n",
    "from diffuse.base_forward_model import MeasurementState, ForwardModel\n",
    "from dataclasses import dataclass\n",
    "from jax.numpy import Array\n",
    "\n",
    "# Define forward models for FLUX images\n",
    "\n",
    "@dataclass\n",
    "class InpaintingMask(ForwardModel):\n",
    "    \"\"\"Inpainting forward model - masks out regions of the image.\"\"\"\n",
    "    mask_type: str = \"rectangle\"  # \"rectangle\" or \"random\"\n",
    "    mask_ratio: float = 0.5  # Fraction of image to mask\n",
    "    std: float = 0.01\n",
    "\n",
    "    def create_mask(self, img_shape: tuple, key: jax.random.PRNGKey = None) -> Array:\n",
    "        \"\"\"Create a binary mask (1 = keep, 0 = remove).\"\"\"\n",
    "        H, W, C = img_shape\n",
    "\n",
    "        if self.mask_type == \"rectangle\":\n",
    "            # Create rectangular mask in center\n",
    "            mask_h = int(H * self.mask_ratio)\n",
    "            mask_w = int(W * self.mask_ratio)\n",
    "            start_h = (H - mask_h) // 2\n",
    "            start_w = (W - mask_w) // 2\n",
    "\n",
    "            mask = jnp.ones((H, W, C))\n",
    "            mask = mask.at[start_h:start_h+mask_h, start_w:start_w+mask_w, :].set(0.0)\n",
    "\n",
    "        elif self.mask_type == \"random\":\n",
    "            # Random pixel mask\n",
    "            if key is None:\n",
    "                key = jax.random.PRNGKey(42)\n",
    "            mask = jax.random.bernoulli(key, 1 - self.mask_ratio, shape=img_shape).astype(jnp.float32)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def apply(self, img: Array, measurement_state: MeasurementState) -> Array:\n",
    "        \"\"\"Apply mask to image.\"\"\"\n",
    "        mask = measurement_state.mask_history\n",
    "        return img * mask\n",
    "\n",
    "    def restore(self, img: Array, measurement_state: MeasurementState) -> Array:\n",
    "        \"\"\"Return the masked region (for gradient computation).\"\"\"\n",
    "        mask = measurement_state.mask_history\n",
    "        inv_mask = 1 - mask\n",
    "        return img * inv_mask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SuperResolution(ForwardModel):\n",
    "    \"\"\"Super-resolution forward model - downsamples then upsamples image.\"\"\"\n",
    "    scale_factor: int = 4  # Downsampling factor (4x4 = 16x fewer pixels)\n",
    "    std: float = 0.01\n",
    "\n",
    "    def apply(self, img: Array, measurement_state: MeasurementState) -> Array:\n",
    "        \"\"\"Downsample image using average pooling.\"\"\"\n",
    "        H, W, C = img.shape\n",
    "        s = self.scale_factor\n",
    "\n",
    "        # Reshape and average pool\n",
    "        img_down = img.reshape(H//s, s, W//s, s, C).mean(axis=(1, 3))\n",
    "\n",
    "        # Upsample back to original size using nearest neighbor\n",
    "        img_up = jnp.repeat(jnp.repeat(img_down, s, axis=0), s, axis=1)\n",
    "\n",
    "        return img_up\n",
    "\n",
    "    def restore(self, img: Array, measurement_state: MeasurementState) -> Array:\n",
    "        \"\"\"Identity for super-res (no special adjoint needed for DPS).\"\"\"\n",
    "        return img\n",
    "\n",
    "\n",
    "print(\"\u2713 Forward models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Inpainting\n",
    "\n",
    "First, generate a reference image, then mask it and recover the missing regions using DPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a reference image for inpainting\n",
    "print(\"Generating reference image...\")\n",
    "INPAINT_PROMPT = \"A cute cat sitting on a windowsill, photorealistic, detailed fur\"\n",
    "INPAINT_STEPS = 25\n",
    "\n",
    "# Update conditioned network with new prompt\n",
    "conditioned_inpaint = loader.prepare_conditioned_network(\n",
    "    prompt=INPAINT_PROMPT,\n",
    "    negative_prompt=None,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    ")\n",
    "\n",
    "# Generate reference image using best quality settings (DPM++2S + FluxTimer)\n",
    "timer_inpaint = FluxTimer(n_steps=INPAINT_STEPS, eps=1e-3, tf=1.0, shift=1.15, use_dynamic_shift=False)\n",
    "latents_ref = run_generation(\n",
    "    conditioned_inpaint, timer_inpaint, DPMpp2sIntegrator, HEIGHT, WIDTH, INPAINT_STEPS, SEED\n",
    ")\n",
    "img_ref = decode_and_display(latents_ref, loader)\n",
    "\n",
    "# Create inpainting mask and degraded observation\n",
    "inpaint_model = InpaintingMask(mask_type=\"rectangle\", mask_ratio=0.5, std=0.01)\n",
    "mask_inpaint = inpaint_model.create_mask(img_ref.shape)\n",
    "img_masked = img_ref * mask_inpaint\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(img_ref)\n",
    "axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(mask_inpaint)\n",
    "axes[1].set_title(\"Mask (white=keep, black=remove)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(img_masked)\n",
    "axes[2].set_title(\"Masked Observation (50% missing)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Masked region: {(1 - mask_inpaint.mean()):.1%} of pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve inpainting with DPS\n",
    "print(\"Running inpainting with DPS...\")\n",
    "\n",
    "# Create measurement state (in latent space)\n",
    "latents_masked = loader.encode_images(img_masked[None, ...])[0]  # Encode to latent space\n",
    "mask_latent = jax.image.resize(\n",
    "    mask_inpaint,\n",
    "    shape=(latents_masked.shape[0], latents_masked.shape[1], mask_inpaint.shape[2]),\n",
    "    method=\"nearest\"\n",
    ")\n",
    "\n",
    "measurement_inpaint = MeasurementState(\n",
    "    y=latents_masked,\n",
    "    mask_history=mask_latent\n",
    ")\n",
    "\n",
    "# Create DPS denoiser with Heun integrator (good balance of quality and speed)\n",
    "flow_inpaint = Flow(tf=1.0)\n",
    "predictor_inpaint = Predictor(\n",
    "    model=flow_inpaint,\n",
    "    network=conditioned_inpaint.network_fn,\n",
    "    prediction_type=\"velocity\"\n",
    ")\n",
    "\n",
    "# Define inpainting forward model in latent space\n",
    "@dataclass\n",
    "class LatentInpainting(ForwardModel):\n",
    "    std: float = 0.01\n",
    "\n",
    "    def apply(self, x: Array, measurement_state: MeasurementState) -> Array:\n",
    "        mask = measurement_state.mask_history\n",
    "        return x * mask\n",
    "\n",
    "    def restore(self, x: Array, measurement_state: MeasurementState) -> Array:\n",
    "        mask = measurement_state.mask_history\n",
    "        return x * (1 - mask)\n",
    "\n",
    "latent_inpaint_model = LatentInpainting(std=0.01)\n",
    "\n",
    "integrator_inpaint = HeunIntegrator(model=flow_inpaint, timer=timer_inpaint)\n",
    "dps_inpaint = DPSDenoiser(\n",
    "    integrator=integrator_inpaint,\n",
    "    model=flow_inpaint,\n",
    "    predictor=predictor_inpaint,\n",
    "    forward_model=latent_inpaint_model,\n",
    "    x0_shape=latents_masked.shape,\n",
    "    zeta=0.3,  # Guidance strength\n",
    ")\n",
    "\n",
    "# Generate inpainted samples\n",
    "key_inpaint = jax.random.PRNGKey(SEED + 100)\n",
    "state_inpaint, _ = dps_inpaint.generate(\n",
    "    key_inpaint,\n",
    "    measurement_inpaint,\n",
    "    n_steps=INPAINT_STEPS,\n",
    "    n_particles=4,  # Generate 4 diverse solutions\n",
    "    keep_history=False,\n",
    ")\n",
    "\n",
    "# Decode latents to images\n",
    "latents_inpainted = state_inpaint.integrator_state.position\n",
    "imgs_inpainted = loader.decode_latents(latents_inpainted.astype(conditioned_inpaint.dtype))\n",
    "\n",
    "print(f\"\u2713 Generated {len(imgs_inpainted)} inpainted results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inpainting results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(\"Inpainting with DPS: Multiple Solutions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Row 1: Original, Masked, First result\n",
    "axes[0, 0].imshow(img_ref)\n",
    "axes[0, 0].set_title(\"Original\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(img_masked)\n",
    "axes[0, 1].set_title(\"Masked (50% missing)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "axes[0, 2].imshow(imgs_inpainted[0])\n",
    "axes[0, 2].set_title(\"DPS Reconstruction #1\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Row 2: Additional diverse solutions\n",
    "for i in range(3):\n",
    "    if i < len(imgs_inpainted) - 1:\n",
    "        axes[1, i].imshow(imgs_inpainted[i + 1])\n",
    "        axes[1, i].set_title(f\"DPS Reconstruction #{i + 2}\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nNotice how DPS produces plausible completions that:\")\n",
    "print(\"- Preserve visible regions exactly\")\n",
    "print(\"- Fill in missing content coherently\")\n",
    "print(\"- Generate diverse solutions (stochastic sampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Super-Resolution\n",
    "\n",
    "Now let's tackle super-resolution: recovering high-resolution details from a low-resolution image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create super-resolution degradation\n",
    "print(\"Creating super-resolution problem...\")\n",
    "\n",
    "# Use the same reference image or generate a new one\n",
    "SR_PROMPT = \"A mountain landscape at golden hour, highly detailed, sharp, photorealistic\"\n",
    "conditioned_sr = loader.prepare_conditioned_network(\n",
    "    prompt=SR_PROMPT,\n",
    "    negative_prompt=None,\n",
    "    guidance_scale=GUIDANCE_SCALE,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    ")\n",
    "\n",
    "# Generate high-res reference\n",
    "latents_hr = run_generation(\n",
    "    conditioned_sr, timer_inpaint, DPMpp2sIntegrator, HEIGHT, WIDTH, INPAINT_STEPS, SEED + 1\n",
    ")\n",
    "img_hr = decode_and_display(latents_hr, loader)\n",
    "\n",
    "# Create low-resolution observation (4x downsampling)\n",
    "sr_model = SuperResolution(scale_factor=4, std=0.01)\n",
    "img_lr_upsampled = sr_model.apply(img_hr, MeasurementState(y=None, mask_history=None))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(img_hr)\n",
    "axes[0].set_title(\"High-Resolution Original\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img_lr_upsampled)\n",
    "axes[1].set_title(\"4x Downsampled (16x fewer pixels)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Create detail comparison boxes\n",
    "h, w = img_hr.shape[:2]\n",
    "crop = (h//4, h//4, 3*h//4, 3*w//4)  # Center crop\n",
    "axes[2].imshow(img_hr[crop[0]:crop[2], crop[1]:crop[3]])\n",
    "axes[2].set_title(\"Detail (original)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Resolution: {HEIGHT}x{WIDTH} \u2192 {HEIGHT//4}x{WIDTH//4} (4x downsampling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve super-resolution with DPS\n",
    "print(\"Running super-resolution with DPS...\")\n",
    "\n",
    "# Work in latent space\n",
    "latents_lr = loader.encode_images(img_lr_upsampled[None, ...])[0]\n",
    "\n",
    "measurement_sr = MeasurementState(\n",
    "    y=latents_lr,\n",
    "    mask_history=jnp.ones_like(latents_lr)  # Full observation (but downsampled)\n",
    ")\n",
    "\n",
    "# Define super-resolution forward model in latent space\n",
    "@dataclass\n",
    "class LatentSuperRes(ForwardModel):\n",
    "    scale_factor: int = 4\n",
    "    std: float = 0.01\n",
    "\n",
    "    def apply(self, x: Array, measurement_state: MeasurementState) -> Array:\n",
    "        \"\"\"Downsample and upsample in latent space.\"\"\"\n",
    "        H, W, C = x.shape\n",
    "        s = self.scale_factor\n",
    "\n",
    "        # Average pool down\n",
    "        x_down = x.reshape(H//s, s, W//s, s, C).mean(axis=(1, 3))\n",
    "        # Upsample back\n",
    "        x_up = jnp.repeat(jnp.repeat(x_down, s, axis=0), s, axis=1)\n",
    "        return x_up\n",
    "\n",
    "    def restore(self, x: Array, measurement_state: MeasurementState) -> Array:\n",
    "        return x\n",
    "\n",
    "latent_sr_model = LatentSuperRes(scale_factor=4, std=0.01)\n",
    "\n",
    "# Create DPS denoiser\n",
    "flow_sr = Flow(tf=1.0)\n",
    "predictor_sr = Predictor(\n",
    "    model=flow_sr,\n",
    "    network=conditioned_sr.network_fn,\n",
    "    prediction_type=\"velocity\"\n",
    ")\n",
    "\n",
    "integrator_sr = HeunIntegrator(model=flow_sr, timer=timer_inpaint)\n",
    "dps_sr = DPSDenoiser(\n",
    "    integrator=integrator_sr,\n",
    "    model=flow_sr,\n",
    "    predictor=predictor_sr,\n",
    "    forward_model=latent_sr_model,\n",
    "    x0_shape=latents_lr.shape,\n",
    "    zeta=0.5,  # Stronger guidance for super-res\n",
    ")\n",
    "\n",
    "# Generate super-resolved images\n",
    "key_sr = jax.random.PRNGKey(SEED + 200)\n",
    "state_sr, _ = dps_sr.generate(\n",
    "    key_sr,\n",
    "    measurement_sr,\n",
    "    n_steps=INPAINT_STEPS,\n",
    "    n_particles=3,  # Generate 3 variations\n",
    "    keep_history=False,\n",
    ")\n",
    "\n",
    "# Decode\n",
    "latents_sr = state_sr.integrator_state.position\n",
    "imgs_sr = loader.decode_latents(latents_sr.astype(conditioned_sr.dtype))\n",
    "\n",
    "print(f\"\u2713 Generated {len(imgs_sr)} super-resolved results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize super-resolution results with detail crops\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.suptitle(\"Super-Resolution with DPS: 4x Upscaling\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Define crop region for detail view\n",
    "crop = (HEIGHT//4, WIDTH//4, 3*HEIGHT//4, 3*WIDTH//4)\n",
    "\n",
    "# Row 1: Full images\n",
    "axes[0, 0].imshow(img_hr)\n",
    "axes[0, 0].set_title(\"Original High-Res\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(img_lr_upsampled)\n",
    "axes[0, 1].set_title(\"Low-Res (4x downsampled)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "axes[0, 2].imshow(imgs_sr[0])\n",
    "axes[0, 2].set_title(\"DPS Super-Resolved\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Row 2: Detail crops\n",
    "axes[1, 0].imshow(img_hr[crop[0]:crop[2], crop[1]:crop[3]])\n",
    "axes[1, 0].set_title(\"Original (detail)\", fontsize=11)\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 1].imshow(img_lr_upsampled[crop[0]:crop[2], crop[1]:crop[3]])\n",
    "axes[1, 1].set_title(\"Low-Res (detail) - blurry\", fontsize=11)\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].imshow(imgs_sr[0][crop[0]:crop[2], crop[1]:crop[3]])\n",
    "axes[1, 2].set_title(\"DPS (detail) - sharp!\", fontsize=11)\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "# Row 3: Additional variations\n",
    "for i in range(3):\n",
    "    if i < len(imgs_sr):\n",
    "        axes[2, i].imshow(imgs_sr[i])\n",
    "        axes[2, i].set_title(f\"DPS Variation #{i+1}\", fontsize=11)\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nNotice how DPS super-resolution:\")\n",
    "print(\"- Recovers sharp details lost in low-res version\")\n",
    "print(\"- Maintains consistency with downsampled observation\")\n",
    "print(\"- Produces plausible high-frequency content\")\n",
    "print(\"- Generates diverse solutions (hallucinated details)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways: Inverse Problems\n",
    "\n",
    "This section demonstrated the **power of modular sampling for inverse problem research**:\n",
    "\n",
    "**What We Showed:**\n",
    "1. **Inpainting** - Recovered 50% missing pixels with DPS + FLUX\n",
    "2. **Super-resolution** - 4x upscaling with sharp detail recovery\n",
    "3. **Multiple solutions** - Stochastic sampling produces diverse plausible completions\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "\u2705 **20 lines of code** - Define your forward model (measurement operator)  \n",
    "\u2705 **Plug into DPS** - Works with any integrator (Euler, DDIM, Heun, DPM++2S)  \n",
    "\u2705 **Test on FLUX** - Or Stable Diffusion, or your custom model  \n",
    "\u2705 **No training** - Pure sampling research, no model modification\n",
    "\n",
    "**The Research Paradigm:**\n",
    "\n",
    "This is exactly the \"algorithms on top of models\" philosophy in action:\n",
    "- \u2705 **Medical imaging**: Define CT/MRI operator \u2192 immediate reconstruction\n",
    "- \u2705 **Deblurring**: Define blur kernel \u2192 deblur with FLUX  \n",
    "- \u2705 **Compressed sensing**: Define measurement matrix \u2192 recover from few samples\n",
    "- \u2705 **Novel guidance**: Implement custom constraints \u2192 guided generation\n",
    "\n",
    "**From Idea to Results: Hours, Not Weeks**\n",
    "\n",
    "Traditional approach:\n",
    "1. Train model (weeks + $$$)\n",
    "2. Modify training code for your problem\n",
    "3. Retrain with new objective\n",
    "4. Hope it works\n",
    "\n",
    "Diffuse approach:\n",
    "1. Implement `ForwardModel` (20 lines)\n",
    "2. Create `DPSDenoiser` with your integrator\n",
    "3. Call `generate()`\n",
    "4. Done\n",
    "\n",
    "**This is the future of diffusion research** - algorithmic innovation on pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Power of Modular Sampling\n",
    "\n",
    "This notebook demonstrated Diffuse's core philosophy: **separation of concerns enables rapid experimentation**.\n",
    "\n",
    "### What We Explored\n",
    "\n",
    "1. **FLUX as Flow Matching** - velocity field prediction {cite}`flux2024` {cite}`Liu2022` {cite}`Lipman2022` (crash course Eq. :eq:`eq:flow_interpolation`)\n",
    "2. **Timers** - discretization schedules (VpTimer, FluxTimer with M\u00f6bius shift)\n",
    "3. **Integrators** - ODE solvers (DDIM, Euler, Heun, DPM++2S, Euler-Maruyama)\n",
    "4. **Deterministic vs Stochastic** - controlled randomness for diversity\n",
    "5. **Inverse Problems** - inpainting and super-resolution with DPS {cite}`Chung2022`\n",
    "6. **Modularity** - swap components without model retraining\n",
    "\n",
    "### The No-Training Research Paradigm\n",
    "\n",
    "**Key Insight**: Most diffusion research is about **algorithms on top of models**, not training:\n",
    "\n",
    "- \ud83c\udfa8 Image editing (InstructPix2Pix, DiffEdit, Imagic)\n",
    "- \ud83d\uddbc\ufe0f Inpainting & outpainting (RePaint, Blended Diffusion) - **demonstrated in this notebook**\n",
    "- \ud83d\udd0d Inverse problems (DPS, RED-diff, medical imaging, super-resolution) - **demonstrated in this notebook**\n",
    "- \ud83c\udfaf Controllable generation (ControlNet-style, regional control, semantic guidance)\n",
    "- \ud83d\udcd0 Novel sampling methods (better integrators, adaptive schedules, churning)\n",
    "- \u26a1 Distillation & acceleration (consistency models, few-step sampling)\n",
    "- \ud83e\udde9 Compositional generation (multi-condition, spatial control)\n",
    "\n",
    "**All of these can be researched using pre-trained models.** No training required.\n",
    "\n",
    "### Modularity = Research Velocity\n",
    "\n",
    "With Diffuse:\n",
    "- Change one line \u2192 test a new integrator\n",
    "- Swap Timer \u2192 try adaptive scheduling\n",
    "- Replace Denoiser \u2192 add guidance method\n",
    "- Implement ForwardModel (20 lines!) \u2192 solve inverse problem\n",
    "\n",
    "**Test ideas in hours, not weeks.**\n",
    "\n",
    "### Recommended Configurations\n",
    "\n",
    "| Use Case | Timer | Integrator | Notes |\n",
    "|----------|-------|------------|-------|\n",
    "| Fast preview | FluxTimer | Euler/DDIM | Quick iteration |\n",
    "| Balanced | FluxTimer | DDIM | Good default |\n",
    "| Max quality | FluxTimer | DPM++2S/Heun | Best visual results |\n",
    "| Diverse samples | FluxTimer | Euler-Maruyama | Stochastic variation |\n",
    "| Inverse problems | FluxTimer | Heun + DPS | Measurement consistency |\n",
    "\n",
    "**Performance vs Quality:**\n",
    "- DDIM/Euler: Baseline speed, good quality\n",
    "- Heun/DPM++2S: 2x cost, 10-20% quality improvement (often worth it!)\n",
    "- Euler-Maruyama: Same speed as Euler, adds diversity\n",
    "\n",
    "### For Algorithm Developers\n",
    "\n",
    "This framework is built for research on sampling:\n",
    "- \u2705 Novel ODE/SDE solvers \u2192 implement Integrator protocol\n",
    "- \u2705 Adaptive schedules \u2192 implement Timer protocol\n",
    "- \u2705 Guidance methods \u2192 extend Denoiser\n",
    "- \u2705 Inverse problems \u2192 implement ForwardModel (as shown with inpainting/super-res!)\n",
    "- \u2705 Works with FLUX, SD, custom models \u2192 just load weights\n",
    "\n",
    "### The Algorithmic Frontier\n",
    "\n",
    "The future of diffusion research is in **algorithms built on top** of pre-trained models. With Diffuse, you can:\n",
    "\n",
    "**Experiment rapidly** - test sampling ideas without retraining  \n",
    "**Focus on research** - no loading complexity, simple pipeline  \n",
    "**Build on SOTA** - use FLUX, Stable Diffusion, or your own models  \n",
    "**Prototype fast** - ideas to results in hours, not weeks\n",
    "\n",
    "**From this notebook:**\n",
    "- Defined inpainting forward model: **20 lines**\n",
    "- Defined super-res forward model: **20 lines**  \n",
    "- Tested on FLUX: **immediately**\n",
    "- Results: **publication-ready**\n",
    "\n",
    "**No training tax. Just algorithms.**\n",
    "\n",
    "This framework supports research on sampling algorithms, guidance methods, inverse problems, and compositional generation. All without touching model weights.\n",
    "\n",
    "Want to add your own model? Check out the GitHub repo for integration guides.  \n",
    "Want to implement your own inverse problem? Copy the ForwardModel examples from this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release model components\n",
    "loader.release_transformer()\n",
    "loader.release_vae()\n",
    "print(\"\u2713 Model components released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}